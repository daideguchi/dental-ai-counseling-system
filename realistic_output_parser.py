#!/usr/bin/env python3
"""
Nottaã¨PLAUD NOTEã®å®Ÿéš›ã®å‡ºåŠ›å½¢å¼ã«å¯¾å¿œã—ãŸãƒ‘ãƒ¼ã‚µãƒ¼
æ­£ç¢ºãªå‡ºåŠ›å½¢å¼ã«åŸºã¥ã„ãŸå‡¦ç†ã‚’å®Ÿè£…
"""

import csv
import json
import re
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import os

class RealisticOutputParser:
    """å®Ÿéš›ã®å‡ºåŠ›å½¢å¼ã«å¯¾å¿œã—ãŸãƒ‘ãƒ¼ã‚µãƒ¼ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.supported_formats = {
            'plaud_note': ['txt', 'srt', 'docx', 'pdf', 'md'],
            'notta': ['txt', 'docx', 'srt', 'pdf', 'csv', 'xlsx']
        }
    
    def parse_plaud_note_txt(self, file_path: str) -> Dict:
        """
        PLAUD NOTE TXTå½¢å¼ã®è§£æ
        ç‰¹å¾´: è©±è€…åŒºåˆ¥ãªã—ã€æ®µè½åŒºåˆ‡ã‚Šã§ç™ºè¨€ã‚’åˆ†é›¢
        """
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read().strip()
        
        # æ®µè½ã§åˆ†å‰²ï¼ˆPLAUD NOTEã®ç‰¹å¾´ï¼‰
        paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
        
        parsed_data = {
            'source': 'plaud_note',
            'format': 'txt',
            'total_segments': len(paragraphs),
            'has_speaker_labels': False,
            'has_timestamps': False,
            'segments': []
        }
        
        # è©±è€…ã‚’æ¨å®šï¼ˆåŒ»å¸«/æ‚£è€…ã®äº¤äº’ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä»®å®šï¼‰
        for i, paragraph in enumerate(paragraphs):
            # ç°¡å˜ãªè©±è€…æ¨å®šãƒ­ã‚¸ãƒƒã‚¯
            speaker = self._estimate_speaker_from_content(paragraph, i)
            
            parsed_data['segments'].append({
                'sequence': i + 1,
                'speaker': speaker,
                'speaker_label': f"Speaker {1 if speaker == 'doctor' else 2}",
                'text': paragraph,
                'timestamp_start': None,
                'timestamp_end': None,
                'confidence': 0.85  # PLAUD NOTEã®æ¨å®šä¿¡é ¼åº¦
            })
        
        return parsed_data
    
    def parse_plaud_note_markdown(self, file_path: str) -> Dict:
        """
        PLAUD NOTE Markdownå½¢å¼ã®è§£æï¼ˆè¦ç´„ãƒ‡ãƒ¼ã‚¿ï¼‰
        ç‰¹å¾´: æ§‹é€ åŒ–ã•ã‚ŒãŸè¦ç´„æƒ…å ±
        """
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Markdownã®æ§‹é€ ã‚’è§£æ
        sections = self._parse_markdown_sections(content)
        
        return {
            'source': 'plaud_note',
            'format': 'markdown_summary',
            'sections': sections,
            'is_summary': True,
            'structured_data': self._extract_structured_summary(sections)
        }
    
    def parse_notta_csv(self, file_path: str) -> Dict:
        """
        Notta CSVå½¢å¼ã®è§£æ
        ç‰¹å¾´: Speaker, Start Time, End Time, Duration, Textåˆ—ã‚’æŒã¤
        """
        parsed_data = {
            'source': 'notta',
            'format': 'csv',
            'has_speaker_labels': True,
            'has_timestamps': True,
            'segments': []
        }
        
        with open(file_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            
            for i, row in enumerate(reader):
                # Nottaã®æ¨™æº–çš„ãªCSVåˆ—åã«å¯¾å¿œ
                speaker_raw = row.get('Speaker', f'Speaker {i % 2 + 1}')
                start_time = row.get('Start Time', '')
                end_time = row.get('End Time', '')
                duration = row.get('Duration', '')
                text = row.get('Text', '')
                
                # è©±è€…ã‚’doctor/patientã«å¤‰æ›
                speaker = self._convert_notta_speaker(speaker_raw, text)
                
                parsed_data['segments'].append({
                    'sequence': i + 1,
                    'speaker': speaker,
                    'speaker_label': speaker_raw,
                    'text': text,
                    'timestamp_start': start_time,
                    'timestamp_end': end_time,
                    'duration': duration,
                    'confidence': 0.92  # Nottaã®å¹³å‡ä¿¡é ¼åº¦
                })
        
        parsed_data['total_segments'] = len(parsed_data['segments'])
        return parsed_data
    
    def parse_notta_xlsx_simulation(self, file_path: str) -> Dict:
        """
        Notta XLSXå½¢å¼ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è§£æ
        å®Ÿéš›ã®XLSXãƒ•ã‚¡ã‚¤ãƒ«ã®ä»£ã‚ã‚Šã«ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è§£æ
        """
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒ‡ãƒ¼ã‚¿è¡Œã‚’æŠ½å‡º
        rows = []
        for line in content.split('\n'):
            if line.startswith('Row '):
                # "Row 1: Speaker 1, 00:00:05, ..." ã®å½¢å¼ã‚’è§£æ
                parts = line.split(': ', 1)[1].split(', ')
                if len(parts) >= 6:
                    rows.append({
                        'Speaker': parts[0],
                        'Start Time': parts[1],
                        'End Time': parts[2],
                        'Duration': parts[3],
                        'Text': parts[4],
                        'Confidence Score': float(parts[5]) if parts[5].replace('.', '').isdigit() else 0.9
                    })
        
        parsed_data = {
            'source': 'notta',
            'format': 'xlsx',
            'has_speaker_labels': True,
            'has_timestamps': True,
            'has_confidence_scores': True,
            'segments': []
        }
        
        for i, row in enumerate(rows):
            speaker = self._convert_notta_speaker(row['Speaker'], row['Text'])
            
            parsed_data['segments'].append({
                'sequence': i + 1,
                'speaker': speaker,
                'speaker_label': row['Speaker'],
                'text': row['Text'],
                'timestamp_start': row['Start Time'],
                'timestamp_end': row['End Time'],
                'duration': row['Duration'],
                'confidence': row['Confidence Score']
            })
        
        parsed_data['total_segments'] = len(parsed_data['segments'])
        return parsed_data
    
    def _estimate_speaker_from_content(self, text: str, sequence: int) -> str:
        """
        ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ã‹ã‚‰è©±è€…ã‚’æ¨å®š
        PLAUD NOTEã¯è©±è€…ãƒ©ãƒ™ãƒ«ãŒãªã„ãŸã‚ã€å†…å®¹ã‹ã‚‰æ¨å®š
        """
        # åŒ»å¸«ã‚‰ã—ã„è¡¨ç¾
        doctor_patterns = [
            r'è¨ºå¯Ÿ', r'æ¤œæŸ»', r'æ²»ç™‚', r'å‡¦ç½®', r'è–¬', r'ç—‡çŠ¶', r'è¨ºæ–­',
            r'ã§ã¯.*ã¾ã—ã‚‡ã†', r'.*ã§ã™ã­', r'.*èªã‚ã¾ã™', r'.*ã¨æ€ã‚ã‚Œã¾ã™',
            r'å±€æ‰€éº»é…”', r'ã†è•', r'æ­¯é«„', r'å……å¡«', r'æŠœæ­¯'
        ]
        
        # æ‚£è€…ã‚‰ã—ã„è¡¨ç¾
        patient_patterns = [
            r'ç—›ã„', r'ã—ã¿ã‚‹', r'æ°—ã«ãªã‚‹', r'å¿ƒé…', r'ä¸å®‰',
            r'ã„ã¤é ƒ', r'ã©ã®ãã‚‰ã„', r'ãŠé¡˜ã„ã—ã¾ã™', r'åˆ†ã‹ã‚Šã¾ã—ãŸ',
            r'.*ã‚“ã§ã™', r'.*ã§ã—ã‚‡ã†ã‹'
        ]
        
        doctor_score = sum(1 for pattern in doctor_patterns if re.search(pattern, text))
        patient_score = sum(1 for pattern in patient_patterns if re.search(pattern, text))
        
        if doctor_score > patient_score:
            return 'doctor'
        elif patient_score > doctor_score:
            return 'patient'
        else:
            # ã‚¹ã‚³ã‚¢ãŒåŒã˜å ´åˆã¯äº¤äº’ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä»®å®š
            return 'doctor' if sequence % 2 == 0 else 'patient'
    
    def _convert_notta_speaker(self, speaker_label: str, text: str) -> str:
        """
        Nottaã®Speaker 1/2ã‚’åŒ»å¸«/æ‚£è€…ã«å¤‰æ›
        """
        # ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ã‚‚è€ƒæ…®ã—ã¦åˆ¤å®š
        if 'Speaker 1' in speaker_label:
            # Speaker 1ãŒåŒ»å¸«ã®å¯èƒ½æ€§ãŒé«˜ã„å ´åˆã®åˆ¤å®š
            if any(word in text for word in ['è¨ºå¯Ÿ', 'æ²»ç™‚', 'è–¬', 'ç—‡çŠ¶', 'è¨ºæ–­']):
                return 'doctor'
            else:
                # å†…å®¹ã‹ã‚‰åˆ¤æ–­ã§ããªã„å ´åˆã¯Speaker 1ã‚’åŒ»å¸«ã¨ä»®å®š
                return 'doctor'
        else:
            return 'patient'
    
    def _parse_markdown_sections(self, content: str) -> Dict:
        """Markdownã®æ§‹é€ ã‚’è§£æ"""
        sections = {}
        current_section = None
        current_content = []
        
        for line in content.split('\n'):
            if line.startswith('## '):
                if current_section:
                    sections[current_section] = '\n'.join(current_content).strip()
                current_section = line[3:].strip()
                current_content = []
            elif line.startswith('# '):
                sections['title'] = line[2:].strip()
            else:
                current_content.append(line)
        
        if current_section:
            sections[current_section] = '\n'.join(current_content).strip()
        
        return sections
    
    def _extract_structured_summary(self, sections: Dict) -> Dict:
        """è¦ç´„ã‹ã‚‰æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        structured = {
            'chief_complaint': '',
            'findings': '',
            'diagnosis': '',
            'treatment_plan': '',
            'patient_instructions': ''
        }
        
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³åã‹ã‚‰å¯¾å¿œã™ã‚‹é …ç›®ã«ãƒãƒƒãƒ”ãƒ³ã‚°
        section_mapping = {
            'ä¸»ãªå†…å®¹': 'chief_complaint',
            'è¨ºå¯Ÿæ‰€è¦‹': 'findings',
            'è¨ºæ–­': 'diagnosis',
            'æ²»ç™‚è¨ˆç”»': 'treatment_plan',
            'æ‚£è€…æŒ‡å°': 'patient_instructions'
        }
        
        for section_name, content in sections.items():
            if section_name in section_mapping:
                structured[section_mapping[section_name]] = content
        
        return structured
    
    def convert_to_unified_format(self, parsed_data: Dict) -> Dict:
        """
        å„å½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚’çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›
        ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ç”¨ã®æ¨™æº–å½¢å¼
        """
        unified = {
            'metadata': {
                'source_system': parsed_data['source'],
                'original_format': parsed_data['format'],
                'total_segments': parsed_data.get('total_segments', 0),
                'has_timestamps': parsed_data.get('has_timestamps', False),
                'has_speaker_labels': parsed_data.get('has_speaker_labels', False),
                'processing_timestamp': datetime.now().isoformat()
            },
            'conversation': [],
            'summary_data': parsed_data.get('structured_data', {})
        }
        
        # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’çµ±ä¸€å½¢å¼ã«å¤‰æ›
        for segment in parsed_data.get('segments', []):
            unified_segment = {
                'sequence': segment['sequence'],
                'speaker': segment['speaker'],
                'speaker_label': segment.get('speaker_label', ''),
                'text': segment['text'],
                'timestamp_start': segment.get('timestamp_start'),
                'timestamp_end': segment.get('timestamp_end'),
                'duration': segment.get('duration'),
                'confidence_score': segment.get('confidence', 0.9)
            }
            unified['conversation'].append(unified_segment)
        
        return unified

def demo_realistic_parsing():
    """å®Ÿéš›ã®å‡ºåŠ›å½¢å¼ã‚’ä½¿ã£ãŸãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    print("ğŸ¯ å®Ÿéš›ã®å‡ºåŠ›å½¢å¼å¯¾å¿œãƒ‘ãƒ¼ã‚µãƒ¼ - ãƒ‡ãƒ¢")
    print("=" * 60)
    
    parser = RealisticOutputParser()
    
    # 1. PLAUD NOTE TXTå½¢å¼
    print("\nğŸ“„ PLAUD NOTE TXTå½¢å¼ã®è§£æ:")
    plaud_txt = parser.parse_plaud_note_txt('realistic_sample_data/plaud_note_transcript.txt')
    print(f"  - ç·ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°: {plaud_txt['total_segments']}")
    print(f"  - è©±è€…ãƒ©ãƒ™ãƒ«: {plaud_txt['has_speaker_labels']}")
    print(f"  - ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—: {plaud_txt['has_timestamps']}")
    
    # 2. PLAUD NOTE Markdownè¦ç´„
    print("\nğŸ“ PLAUD NOTE Markdownè¦ç´„ã®è§£æ:")
    plaud_md = parser.parse_plaud_note_markdown('realistic_sample_data/plaud_note_summary.md')
    print(f"  - ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ•°: {len(plaud_md['sections'])}")
    print(f"  - æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿: {len(plaud_md['structured_data'])}é …ç›®")
    
    # 3. Notta CSVå½¢å¼
    print("\nğŸ“Š Notta CSVå½¢å¼ã®è§£æ:")
    notta_csv = parser.parse_notta_csv('realistic_sample_data/notta_transcript.csv')
    print(f"  - ç·ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°: {notta_csv['total_segments']}")
    print(f"  - è©±è€…ãƒ©ãƒ™ãƒ«: {notta_csv['has_speaker_labels']}")
    print(f"  - ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—: {notta_csv['has_timestamps']}")
    
    # 4. Notta XLSXå½¢å¼ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
    print("\nğŸ“ˆ Notta XLSXå½¢å¼ã®è§£æ:")
    notta_xlsx = parser.parse_notta_xlsx_simulation('realistic_sample_data/notta_transcript.xlsx')
    print(f"  - ç·ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°: {notta_xlsx['total_segments']}")
    print(f"  - ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢: {notta_xlsx['has_confidence_scores']}")
    
    # 5. çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¸ã®å¤‰æ›
    print("\nğŸ”„ çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¸ã®å¤‰æ›:")
    unified_plaud = parser.convert_to_unified_format(plaud_txt)
    unified_notta = parser.convert_to_unified_format(notta_csv)
    
    print(f"  - PLAUD NOTEçµ±ä¸€å½¢å¼: {len(unified_plaud['conversation'])}ç™ºè¨€")
    print(f"  - Nottaçµ±ä¸€å½¢å¼: {len(unified_notta['conversation'])}ç™ºè¨€")
    
    # 6. çµæœã‚’JSONã§å‡ºåŠ›
    os.makedirs('output/realistic_parsing', exist_ok=True)
    
    with open('output/realistic_parsing/plaud_unified.json', 'w', encoding='utf-8') as f:
        json.dump(unified_plaud, f, ensure_ascii=False, indent=2)
    
    with open('output/realistic_parsing/notta_unified.json', 'w', encoding='utf-8') as f:
        json.dump(unified_notta, f, ensure_ascii=False, indent=2)
    
    print("\nâœ… è§£æçµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›:")
    print("  - output/realistic_parsing/plaud_unified.json")
    print("  - output/realistic_parsing/notta_unified.json")
    
    # 7. æ¯”è¼ƒåˆ†æ
    print("\nğŸ“Š å½¢å¼åˆ¥æ¯”è¼ƒåˆ†æ:")
    print(f"  PLAUD NOTE TXT:")
    print(f"    - åˆ©ç‚¹: ã‚·ãƒ³ãƒ—ãƒ«ã€è»½é‡")
    print(f"    - æ¬ ç‚¹: è©±è€…åŒºåˆ¥ãªã—ã€ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãªã—")
    print(f"    - æ¨å®šç²¾åº¦: ä¸­ç¨‹åº¦ï¼ˆå†…å®¹ãƒ™ãƒ¼ã‚¹æ¨å®šï¼‰")
    
    print(f"  Notta CSV:")
    print(f"    - åˆ©ç‚¹: è©±è€…åŒºåˆ¥ã€ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã€æ§‹é€ åŒ–")
    print(f"    - æ¬ ç‚¹: ä¿¡é ¼åº¦æƒ…å ±ãªã—")
    print(f"    - æ¨å®šç²¾åº¦: é«˜ï¼ˆæ˜ç¤ºçš„ãƒ©ãƒ™ãƒ«ï¼‰")
    
    print(f"  Notta XLSX:")
    print(f"    - åˆ©ç‚¹: å…¨æƒ…å ±å«æœ‰ã€ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢")
    print(f"    - æ¬ ç‚¹: ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºå¤§ã€å‡¦ç†è¤‡é›‘")
    print(f"    - æ¨å®šç²¾åº¦: æœ€é«˜ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿åˆ©ç”¨å¯èƒ½ï¼‰")

if __name__ == "__main__":
    demo_realistic_parsing()